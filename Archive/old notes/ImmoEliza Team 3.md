---
marp: true
theme: uncover
---
title: "\U0001F3E0 ImmoEliza â€” Team 3"

---



---

# ğŸ  ImmoEliza â€” Team 3

---
> **Get the Data**
> -  Scrape data from a property page â†’ write to file
> -  Scrape all property pages by calling `scrape_data()`
>       -  âš ï¸ This needs all the property links!

> **Knowing What Data to Get**
> -  Get the property links from a search page â†’ write to file
> -  Get all the search pages

> **Data Manipulation**
> -  Clean the data (remove duplicates)
> - Expect crashes â†’ Write data to file

---

## First Challenge

### **Getting past 50 pages of results**

Build a new URL by changing the page number

---

## First Idea: Filter by Province

> â— More than 1000 results for most provinces
>
> â†’ Forward and reverse trick

- Still 4 provinces with more than 2000 results
  - Slice the data further? (subtypes, pricesâ€¦)
    - â†’ **Inelegant, messy, hard to automateâ€¦**

---

## Better Idea: Filter by Zip Code

- Trick the site's API to give out all `{"zip_code": "city"}`
- Build a function to generate search page URLs
- Test it works
- â†’ `All_Search_Pages.json`
- â†’ `get_property_links_from_a_search_page()`
- â†’ **ALL property links collected** âœ…

---

## Second Challenge: Cleaning Data

**JSON with duplicates â†’ New JSON without duplicates**

---

## Avoiding Crashes & Data Manipulation

- **Challenge:** Avoid data crashes
  - â†’ Write each scraped line to a JSON

- **Manipulate data:**
  - â†’ JSON to CSV converter

---

## Ultimate Challenge

### â€¦solving the GitHub messâ€¦

---

## Solution

- Ask colleagues ğŸ¤

---

## Q&A

**How do you organise your Repo?**